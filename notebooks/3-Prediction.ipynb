{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcc0ee0",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdb424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd  # data analysis\n",
    "import numpy as np  # mathematic evaluations\n",
    "#from sklearn.preprocessing import MinMaxScaler # for data scaling\n",
    "#from sklearn.preprocessing import RobustScaler # for data scaling\n",
    "from sklearn.preprocessing import StandardScaler # for data scaling\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.utils.class_weight import compute_sample_weight # for changing weight of the columns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "#from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import seaborn as sns # data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b2e9ac",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d472a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv('C:/Users/lluis/Desktop/Documents/IronHack/Final_Project/data/cleaned/df_merged.csv')\n",
    "df_exchange_rate = pd.read_csv('C:/Users/lluis/Desktop/Documents/IronHack/Final_Project/data/cleaned/df_exchange_rate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3bf4c0",
   "metadata": {},
   "source": [
    "### Transforming Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480a70e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 59 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   country_name      175 non-null    object \n",
      " 1   un_class_2014     150 non-null    object \n",
      " 2   imf_class_2023    172 non-null    object \n",
      " 3   g7                175 non-null    bool   \n",
      " 4   eu_member         175 non-null    bool   \n",
      " 5   fuel_exp_country  175 non-null    bool   \n",
      " 6   wealth_rank       161 non-null    float64\n",
      " 7   ISO2              174 non-null    object \n",
      " 8   ISO3              175 non-null    object \n",
      " 9   1980              126 non-null    float64\n",
      " 10  1981              127 non-null    float64\n",
      " 11  1982              127 non-null    float64\n",
      " 12  1983              127 non-null    float64\n",
      " 13  1984              127 non-null    float64\n",
      " 14  1985              128 non-null    float64\n",
      " 15  1986              129 non-null    float64\n",
      " 16  1987              129 non-null    float64\n",
      " 17  1988              129 non-null    float64\n",
      " 18  1989              129 non-null    float64\n",
      " 19  1990              140 non-null    float64\n",
      " 20  1991              142 non-null    float64\n",
      " 21  1992              153 non-null    float64\n",
      " 22  1993              155 non-null    float64\n",
      " 23  1994              156 non-null    float64\n",
      " 24  1995              158 non-null    float64\n",
      " 25  1996              159 non-null    float64\n",
      " 26  1997              161 non-null    float64\n",
      " 27  1998              164 non-null    float64\n",
      " 28  1999              164 non-null    float64\n",
      " 29  2000              168 non-null    float64\n",
      " 30  2001              168 non-null    float64\n",
      " 31  2002              170 non-null    float64\n",
      " 32  2003              171 non-null    float64\n",
      " 33  2004              173 non-null    float64\n",
      " 34  2005              173 non-null    float64\n",
      " 35  2006              173 non-null    float64\n",
      " 36  2007              173 non-null    float64\n",
      " 37  2008              173 non-null    float64\n",
      " 38  2009              173 non-null    float64\n",
      " 39  2010              174 non-null    float64\n",
      " 40  2011              173 non-null    float64\n",
      " 41  2012              174 non-null    float64\n",
      " 42  2013              174 non-null    float64\n",
      " 43  2014              174 non-null    float64\n",
      " 44  2015              174 non-null    float64\n",
      " 45  2016              174 non-null    float64\n",
      " 46  2017              174 non-null    float64\n",
      " 47  2018              174 non-null    float64\n",
      " 48  2019              174 non-null    float64\n",
      " 49  2020              173 non-null    float64\n",
      " 50  2021              173 non-null    float64\n",
      " 51  2022              173 non-null    float64\n",
      " 52  2023              170 non-null    float64\n",
      " 53  2024              170 non-null    float64\n",
      " 54  2025              170 non-null    float64\n",
      " 55  2026              169 non-null    float64\n",
      " 56  2027              169 non-null    float64\n",
      " 57  2028              169 non-null    float64\n",
      " 58  2029              169 non-null    float64\n",
      "dtypes: bool(3), float64(51), object(5)\n",
      "memory usage: 77.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632e07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I am dropping the columns I will not use\n",
    "\n",
    "df_merged_pred = df_merged.drop(['un_class_2014', 'g7', 'eu_member', 'fuel_exp_country', 'wealth_rank', 'imf_class_2023', 'ISO2', 'ISO3', '2025', '2026', '2027', '2028','2029'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df4c6b",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99fd7c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lluis\\AppData\\Local\\Temp\\ipykernel_11128\\485477642.py:2: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_merged_pred = df_merged_pred.interpolate(axis=0)\n"
     ]
    }
   ],
   "source": [
    "# We will estimate the NaN values based on other values in the same column (year)\n",
    "df_merged_pred = df_merged_pred.interpolate(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a65378",
   "metadata": {},
   "source": [
    "### Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cac621",
   "metadata": {},
   "source": [
    "Selecting the countries for which I want to predict GDP per capita with ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6be6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with only rows where 'country_name' is 'Spain'\n",
    "df_spain = df_merged_pred[df_merged_pred['country_name'] == 'Spain'].T\n",
    "# Drop the first row\n",
    "df_spain = df_spain.drop(df_spain.index[0])\n",
    "\n",
    "# Create a new DataFrame with only rows where 'country_name' is 'Switzerland'\n",
    "df_switzerland = df_merged_pred[df_merged_pred['country_name'] == 'Switzerland'].T\n",
    "# Drop the first row\n",
    "df_switzerland = df_switzerland.drop(df_switzerland.index[0])\n",
    "\n",
    "# Create a new DataFrame with only rows where 'country_name' is 'United States'\n",
    "df_usa = df_merged_pred[df_merged_pred['country_name'] == 'United States'].T\n",
    "# Drop the first row\n",
    "df_usa = df_usa.drop(df_usa.index[0])\n",
    "\n",
    "# Create a new DataFrame with only rows where 'country_name' is 'India'\n",
    "df_india = df_merged_pred[df_merged_pred['country_name'] == 'India'].T\n",
    "# Drop the first row\n",
    "df_india = df_india.drop(df_india.index[0])\n",
    "\n",
    "# Create a new DataFrame with only rows where 'country_name' is 'Venezuela'\n",
    "df_venezuela = df_merged_pred[df_merged_pred['country_name'] == 'Venezuela'].T\n",
    "# Drop the first row\n",
    "df_venezuela = df_venezuela.drop(df_venezuela.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f913fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance for each of the countries\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "scaler3 = StandardScaler()\n",
    "scaler4 = StandardScaler()\n",
    "scaler5 = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "df_spain_scaled = pd.DataFrame(scaler1.fit_transform(df_spain), columns=df_spain.columns, index=df_spain.index)\n",
    "df_switzerland_scaled = pd.DataFrame(scaler2.fit_transform(df_switzerland), columns=df_switzerland.columns, index=df_switzerland.index)\n",
    "df_usa_scaled = pd.DataFrame(scaler3.fit_transform(df_usa), columns=df_usa.columns, index=df_usa.index)\n",
    "df_india_scaled = pd.DataFrame(scaler4.fit_transform(df_india), columns=df_india.columns, index=df_india.index)\n",
    "df_venezuela_scaled = pd.DataFrame(scaler5.fit_transform(df_venezuela), columns=df_venezuela.columns, index=df_venezuela.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d28fe1",
   "metadata": {},
   "source": [
    "### ARIMA Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fa511",
   "metadata": {},
   "source": [
    "ARIMA prediction for Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f40653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=-26.474, Time=32.44 sec\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=-27.690, Time=0.03 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=-28.411, Time=13.27 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=-28.460, Time=46.65 sec\n",
      " ARIMA(0,1,1)(0,1,0)[12]             : AIC=-28.280, Time=4.59 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=-26.757, Time=46.84 sec\n",
      " ARIMA(0,1,1)(0,1,2)[12]             : AIC=-26.757, Time=39.01 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=-28.740, Time=17.16 sec\n",
      " ARIMA(0,1,1)(2,1,0)[12]             : AIC=-26.757, Time=28.62 sec\n",
      " ARIMA(0,1,1)(2,1,1)[12]             : AIC=-24.757, Time=50.75 sec\n",
      " ARIMA(0,1,0)(1,1,0)[12]             : AIC=-28.539, Time=9.90 sec\n",
      " ARIMA(1,1,1)(1,1,0)[12]             : AIC=-26.763, Time=55.43 sec\n",
      " ARIMA(0,1,2)(1,1,0)[12]             : AIC=-26.775, Time=23.30 sec\n",
      " ARIMA(1,1,2)(1,1,0)[12]             : AIC=-25.023, Time=80.91 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12] intercept   : AIC=-27.748, Time=26.44 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(1,1,0)[12]          \n",
      "Total fit time: 475.349 seconds\n",
      "                                      SARIMAX Results                                      \n",
      "===========================================================================================\n",
      "Dep. Variable:                                   y   No. Observations:                   45\n",
      "Model:             SARIMAX(0, 1, 1)x(1, 1, [], 12)   Log Likelihood                  17.370\n",
      "Date:                             Thu, 06 Jun 2024   AIC                            -28.740\n",
      "Time:                                     16:57:08   BIC                            -24.343\n",
      "Sample:                                 01-01-1980   HQIC                           -27.283\n",
      "                                      - 01-01-2024                                         \n",
      "Covariance Type:                               opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "ma.L1          0.2726      0.102      2.663      0.008       0.072       0.473\n",
      "ar.S.L12      -0.3847      0.334     -1.153      0.249      -1.039       0.269\n",
      "sigma2         0.0186      0.003      5.433      0.000       0.012       0.025\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.03   Jarque-Bera (JB):                50.34\n",
      "Prob(Q):                              0.86   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):              67.36   Skew:                             0.43\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                         9.08\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "The predicted next 5 values are 2025-01-01    2.281613\n",
      "2026-01-01    2.359394\n",
      "2027-01-01    2.456647\n",
      "2028-01-01    2.614866\n",
      "2029-01-01    2.777804\n",
      "Freq: YS-JAN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# We want to predict the next 5 columns of df_spain\n",
    "y = df_spain_scaled.iloc[:, -1]\n",
    "\n",
    "# Define the auto_arima model\n",
    "model = auto_arima(y, start_p=1, start_q=1, max_p=6, max_q=6, m=12,\n",
    "                   start_P=0, seasonal=True, d=1, D=1, trace=True,\n",
    "                   error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit(y)\n",
    "\n",
    "# Print the best model parameters\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Predict the next 5 values\n",
    "next_values = model_fit.predict(n_periods=5)\n",
    "print(f\"The predicted next 5 values are {next_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4720c2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lluis\\AppData\\Local\\Temp\\ipykernel_11128\\873123723.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train, y_test = y[train_index], y[test_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.2501697625435116\n",
      "MSE: 0.08214260209910619\n",
      "RMSE: 0.2866053071719123\n",
      "R2 Score: 0.4599458440052079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lluis\\AppData\\Local\\Temp\\ipykernel_11128\\873123723.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train, y_test = y[train_index], y[test_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.2415005015967703\n",
      "MSE: 0.11679653056011245\n",
      "RMSE: 0.3417550739347003\n",
      "R2 Score: 0.6146746418650428\n"
     ]
    }
   ],
   "source": [
    "# Define the TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "# Loop over the splits\n",
    "for train_index, test_index in tscv.split(y):\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model_fit = model.fit(y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model_fit.predict(n_periods=len(y_test))\n",
    "\n",
    "    # Calculate the metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fe03d",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "- MAE, MSE, and RMSE: These metrics appear to be in a reasonable range, but their adequacy depends on the context, including the scale of the data and the specific problem domain. In isolation, they don't tell you whether the model is good or bad; they need to be compared to the baseline errors (e.g., errors obtained from a simple model like mean prediction).\n",
    "\n",
    "- R² Score: The negative R² score is concerning. It suggests that the model is not performing well and is worse than a trivial model that would predict the mean of the target variable for all observations. This indicates that there may be significant issues with the model, such as overfitting, underfitting, incorrect model assumptions, or issues with the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c03444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53196.52168144]\n",
      " [54130.05591264]\n",
      " [55297.28060515]\n",
      " [57196.2362547 ]\n",
      " [59151.81642286]]\n"
     ]
    }
   ],
   "source": [
    "# Now to descale your predictions\n",
    "next_values_descaled = scaler1.inverse_transform(next_values.values.reshape(-1, 1))\n",
    "\n",
    "print(next_values_descaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8789d",
   "metadata": {},
   "source": [
    "We add the predicted data back to spain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e164d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add our predictions to the dataframe\n",
    "years = [2025, 2026, 2027, 2028, 2029]\n",
    "for i, year in enumerate(years):\n",
    "    df_spain.loc[year] = next_values_descaled[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf63cd",
   "metadata": {},
   "source": [
    "ARIMA prediction for Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290050c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=-51.219, Time=70.34 sec\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=-47.046, Time=0.03 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=-51.311, Time=21.94 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=-52.861, Time=28.44 sec\n",
      " ARIMA(0,1,1)(0,1,0)[12]             : AIC=-49.433, Time=3.61 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=-51.422, Time=41.45 sec\n",
      " ARIMA(0,1,1)(0,1,2)[12]             : AIC=-51.422, Time=78.89 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=-52.960, Time=33.98 sec\n"
     ]
    }
   ],
   "source": [
    "# We want to predict the next 5 columns of df_switzerland\n",
    "y = df_switzerland_scaled.iloc[:, -1]\n",
    "\n",
    "# Define the auto_arima model\n",
    "model = auto_arima(y, start_p=1, start_q=1, max_p=6, max_q=6, m=12,\n",
    "                   start_P=0, seasonal=True, d=1, D=1, trace=True,\n",
    "                   error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit(y)\n",
    "\n",
    "# Print the best model parameters\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Predict the next 5 values\n",
    "next_values = model_fit.predict(n_periods=5)\n",
    "print(f\"The predicted next 5 values are {next_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "# Loop over the splits\n",
    "for train_index, test_index in tscv.split(y):\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model_fit = model.fit(y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model_fit.predict(n_periods=len(y_test))\n",
    "\n",
    "    # Calculate the metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to descale your predictions\n",
    "next_values_descaled = scaler2.inverse_transform(next_values.values.reshape(-1, 1))\n",
    "\n",
    "print(next_values_descaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add our predictions to the dataframe\n",
    "years = [2025, 2026, 2027, 2028, 2029]\n",
    "for i, year in enumerate(years):\n",
    "    df_switzerland.loc[year] = next_values_descaled[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed24d55",
   "metadata": {},
   "source": [
    "ARIMA prediction for USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1826c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to predict the next 5 columns of df_usa\n",
    "y = df_usa_scaled.iloc[:, -1]\n",
    "\n",
    "# Define the auto_arima model\n",
    "model = auto_arima(y, start_p=1, start_q=1, max_p=6, max_q=6, m=12,\n",
    "                   start_P=0, seasonal=True, d=1, D=1, trace=True,\n",
    "                   error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit(y)\n",
    "\n",
    "# Print the best model parameters\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Predict the next 5 values\n",
    "next_values = model_fit.predict(n_periods=5)\n",
    "print(f\"The predicted next 5 values are {next_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "# Loop over the splits\n",
    "for train_index, test_index in tscv.split(y):\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model_fit = model.fit(y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model_fit.predict(n_periods=len(y_test))\n",
    "\n",
    "    # Calculate the metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to descale your predictions\n",
    "next_values_descaled = scaler3.inverse_transform(next_values.values.reshape(-1, 1))\n",
    "\n",
    "print(next_values_descaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad15ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add our predictions to the dataframe\n",
    "years = [2025, 2026, 2027, 2028, 2029]\n",
    "for i, year in enumerate(years):\n",
    "    df_usa.loc[year] = next_values_descaled[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f350cf",
   "metadata": {},
   "source": [
    "ARIMA prediction for India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to predict the next 5 columns of df_india\n",
    "y = df_india_scaled.iloc[:, -1]\n",
    "\n",
    "# Define the auto_arima model\n",
    "model = auto_arima(y, start_p=1, start_q=1, max_p=6, max_q=6, m=12,\n",
    "                   start_P=0, seasonal=True, d=1, D=1, trace=True,\n",
    "                   error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit(y)\n",
    "\n",
    "# Print the best model parameters\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Predict the next 5 values\n",
    "next_values = model_fit.predict(n_periods=5)\n",
    "print(f\"The predicted next 5 values are {next_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac382b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "# Loop over the splits\n",
    "for train_index, test_index in tscv.split(y):\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model_fit = model.fit(y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model_fit.predict(n_periods=len(y_test))\n",
    "\n",
    "    # Calculate the metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to descale your predictions\n",
    "next_values_descaled = scaler4.inverse_transform(next_values.values.reshape(-1, 1))\n",
    "\n",
    "print(next_values_descaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add our predictions to the dataframe\n",
    "years = [2025, 2026, 2027, 2028, 2029]\n",
    "for i, year in enumerate(years):\n",
    "    df_india.loc[year] = next_values_descaled[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273f01a",
   "metadata": {},
   "source": [
    "ARIMA prediction for Venezuela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to predict the next 5 columns of df_venezuela\n",
    "y = df_venezuela_scaled.iloc[:, -1]\n",
    "\n",
    "# Define the auto_arima model\n",
    "model = auto_arima(y, start_p=1, start_q=1, max_p=6, max_q=6, m=12,\n",
    "                   start_P=0, seasonal=True, d=1, D=1, trace=True,\n",
    "                   error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit(y)\n",
    "\n",
    "# Print the best model parameters\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Predict the next 5 values\n",
    "next_values = model_fit.predict(n_periods=5)\n",
    "print(f\"The predicted next 5 values are {next_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model_fit = model.fit(y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model_fit.predict(n_periods=len(y_test))\n",
    "\n",
    "# Calculate the metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to descale your predictions\n",
    "next_values_descaled = scaler4.inverse_transform(next_values.values.reshape(-1, 1))\n",
    "\n",
    "print(next_values_descaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add our predictions to the dataframe\n",
    "years = [2025, 2026, 2027, 2028, 2029]\n",
    "for i, year in enumerate(years):\n",
    "    df_venezuela.loc[year] = next_values_descaled[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa07040",
   "metadata": {},
   "source": [
    "### Predictions Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08b2a1",
   "metadata": {},
   "source": [
    "First we create a pivot table with the forecast of ECB for the next years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of years from 1993 to 2029\n",
    "years = list(map(str, range(1993, 2030)))\n",
    "\n",
    "# List of countries\n",
    "countries = ['Spain', 'United States', 'India', 'Switzerland', 'Venezuela']\n",
    "\n",
    "# Filter the DataFrame to include only the countries in the list\n",
    "df_filtered = df_merged[df_merged['country_name'].isin(countries)]\n",
    "\n",
    "# Set 'country_name' as the index and select only the columns for the years\n",
    "pivot_table_ecb = df_filtered.set_index('country_name')[years]\n",
    "\n",
    "# Display the pivot table\n",
    "print(pivot_table_ecb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff491c",
   "metadata": {},
   "source": [
    "Plotting ECB Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b200b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame\n",
    "df_melted = pd.melt(pivot_table_ecb.reset_index(), id_vars='country_name', var_name='Year', value_name='Value')\n",
    "\n",
    "# Convert 'Year' to numeric\n",
    "df_melted['Year'] = pd.to_numeric(df_melted['Year'])\n",
    "\n",
    "# Create a line plot for the years before 2025\n",
    "sns.lineplot(data=df_melted[df_melted['Year'] < 2025], x='Year', y='Value', hue='country_name')\n",
    "\n",
    "# Create a line plot for the years from 2025 onwards with a different line style\n",
    "sns.lineplot(data=df_melted[df_melted['Year'] >= 2025], x='Year', y='Value', hue='country_name', style=True, dashes=[(2,2)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b1bb0",
   "metadata": {},
   "source": [
    "We create a pivot table with our predictions in the same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c72131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose df_usa\n",
    "df_spain_p = df_spain.T\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992']\n",
    "\n",
    "# Drop the columns\n",
    "df_spain_p = df_spain_p.drop(columns=columns_to_drop)\n",
    "\n",
    "# Add a column named 'country_name' at the beginning and set its value to 'United States'\n",
    "df_spain_p.insert(0, 'country_name', 'Spain')\n",
    "\n",
    "# Set 'country_name' as the index\n",
    "df_spain_p = df_spain_p.set_index('country_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose df_usa\n",
    "df_usa_p = df_usa.T\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992']\n",
    "\n",
    "# Drop the columns\n",
    "df_usa_p = df_usa_p.drop(columns=columns_to_drop)\n",
    "\n",
    "# Add a column named 'country_name' at the beginning and set its value to 'United States'\n",
    "df_usa_p.insert(0, 'country_name', 'United States')\n",
    "\n",
    "# Set 'country_name' as the index\n",
    "df_usa_p = df_usa_p.set_index('country_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose df_switzerland\n",
    "df_switzerland_p = df_switzerland.T\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992']\n",
    "\n",
    "# Drop the columns\n",
    "df_switzerland_p = df_switzerland_p.drop(columns=columns_to_drop)\n",
    "\n",
    "# Add a column named 'country_name' at the beginning and set its value to 'United States'\n",
    "df_switzerland_p.insert(0, 'country_name', 'Switzerland')\n",
    "\n",
    "# Set 'country_name' as the index\n",
    "df_switzerland_p = df_switzerland_p.set_index('country_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d732801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose df_india\n",
    "df_india_p = df_india.T\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992']\n",
    "\n",
    "# Drop the columns\n",
    "df_india_p = df_india_p.drop(columns=columns_to_drop)\n",
    "\n",
    "# Add a column named 'country_name' at the beginning and set its value to 'United States'\n",
    "df_india_p.insert(0, 'country_name', 'India')\n",
    "\n",
    "# Set 'country_name' as the index\n",
    "df_india_p = df_india_p.set_index('country_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2cf185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose df_venezuela\n",
    "df_venezuela_p = df_venezuela.T\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992']\n",
    "\n",
    "# Drop the columns\n",
    "df_venezuela_p = df_venezuela_p.drop(columns=columns_to_drop)\n",
    "\n",
    "# Add a column named 'country_name' at the beginning and set its value to 'United States'\n",
    "df_venezuela_p.insert(0, 'country_name', 'Venezuela')\n",
    "\n",
    "# Set 'country_name' as the index\n",
    "df_venezuela_p = df_venezuela_p.set_index('country_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes\n",
    "df_p = pd.concat([df_spain_p, df_usa_p, df_switzerland_p, df_india_p, df_venezuela_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeccf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_p.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to numerical\n",
    "df_p = df_p.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ae0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame\n",
    "df_melted = pd.melt(df_p.reset_index(), id_vars='country_name', var_name='Year', value_name='Value')\n",
    "\n",
    "# Convert 'Year' to integer\n",
    "df_melted['Year'] = df_melted['Year'].astype(int)\n",
    "\n",
    "# Create a line plot for years before 2025\n",
    "sns.lineplot(data=df_melted[df_melted['Year'] < 2025], x='Year', y='Value', hue='country_name')\n",
    "\n",
    "# Create a dashed line plot for years from 2025 onwards\n",
    "sns.lineplot(data=df_melted[df_melted['Year'] >= 2025], x='Year', y='Value', hue='country_name', style='country_name', dashes=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Melt the DataFrame\n",
    "df_melted = pd.melt(df_p.reset_index(), id_vars='country_name', var_name='Year', value_name='Value')\n",
    "\n",
    "# Convert 'Year' to integer\n",
    "df_melted['Year'] = df_melted['Year'].astype(int)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Create a line plot for all years in the first subplot\n",
    "sns.lineplot(data=df_melted, x='Year', y='Value', hue='country_name', ax=axs[0])\n",
    "axs[0].set_title('All Years')\n",
    "\n",
    "# Create a line plot for years before 2025 and a dashed line plot for years from 2025 onwards in the second subplot\n",
    "sns.lineplot(data=df_melted[df_melted['Year'] < 2025], x='Year', y='Value', hue='country_name', ax=axs[1])\n",
    "sns.lineplot(data=df_melted[df_melted['Year'] >= 2025], x='Year', y='Value', hue='country_name', style='country_name', dashes=True, ax=axs[1])\n",
    "axs[1].set_title('Years < 2025 and Years >= 2025')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388d7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Final_Project",
   "language": "python",
   "name": "final_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
